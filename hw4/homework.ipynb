{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac449644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6065471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\imphi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada5e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3332b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b82ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cde7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5eb52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2c6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ff767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence, n=2))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd51a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {word:i for i, word in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1+' '+word2 \n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fad7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = csr_matrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f56986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        nxt = id2word[chosen]\n",
    "        text.append(nxt)\n",
    "        \n",
    "        bi = id2bigram[current_idx]\n",
    "        prcd = bi.split()[1]\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id[start]\n",
    "        else:\n",
    "            prod = prcd + ' ' + nxt\n",
    "            current_idx = bigram2id[prod]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e657c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " вроде бы отчасти на них влияет  вот это я сама власть и ассортимент завезли  мы даже предсказать не можем  наказание пусть они и увидят это то же что там она не нужна если тян онлайн возможно прямо сейчас возьму себя в божеский вид повернув до нужного угла  так я тоже на познакомился с тян а как это сделала мс так как ты их не два  кризис раннего возраста скромные  пиздос дожили модами теперь допиливают то что у меня у друзей путина и до этого  сан вынужден сотрудничать с вояками  мда походу анимация \n",
      "\n",
      "2 \n",
      " ты ничего не писал что прям никто больше не сделают миллиардеров счастливыми  судить людей по знакомству  они уже один раз цепануть телку и утащить ее в лоб немцетяжи 90 мм м 3 лучше бр-471 б у купи на которой он второй ебали тремя хуями и т д потому что моды пиратские эти безвольные долбоебы все удалят как totw оно и понятно интеграл от коцикла по циклу  на видеоролике было прекрасно  и так далее уже не помню такого за собой не замечаю или игнорю в в трете смотрем чирвоны смотрем внемательна новую шабемку твящ завади чорнава плаща \n",
      "\n",
      "3 \n",
      " ищи такое же право на пользование тележкой ибо тележка тадам  ты ебанутый что бы тот ответил  говно моча блевотина я смотрю ты очень ошибаешься  утром забрал и ушел думаю не хотел чтобы кто-то был рядом можно было выжить на insane как без палева выкупает фрс  имею айфон 4 s хороший пример некачественного продукта  колеса помогут  пока что нет ролевых моделей при слове ученый все представляют как это по сути лучший класс в 80 ых в майями с коксом и начать её трахать в задницу что и компьютер ну такой мы не ради денег которые \n",
      "\n",
      "4 \n",
      " вплоть до поцелуев мужчина отвечает безоговорочной взаимностью  жаропонижающие есть  вдуматься только мой прогноз по доллару-рублю и бренту что там осталось  зачем сожалеть о чем-то за чашечкой чая то это уже блядство  стоит задача пережать большое количество стволов в одном паблике резко сменил тематику суицидальной пропаганды на личностное развитие  экономить в наших влажных мечтах  мы собираем данные диагностики и использования предоставить но некоторые тот же аккорд но чтобы это гарантированно прокнуло  емнип взято прямиком из сирии  сюда вот неделю не хочешь но чтоб там было всё и сразу же за честную игру да \n",
      "\n",
      "5 \n",
      " он ещё тут выебнись  какая з п по рынку  можешь вообще в будущие 2 года но известно о кыштымской аварии »  договор был на середине уже думал он меня однажды чуть не вытекли  но при этом он доработает до конца если предлагает повторить попытку повторяешь если раза три или фиг с ним по-хитрому разобраться  у тебя пидорахинское строение а не в базовом пакете идет быть самым говноедом итт  ну там кошки собаки если нет продолжения ахуенные видео у американцев и логично  произведение мер  первые 2 семестра  и всё было спасибо рыба-кун \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    generated = generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '')\n",
    "    print(k+1, '\\n', generated, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
